{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Pipelines in scikit-learn - Lab \n", "\n", "## Introduction \n", "\n", "In this lab you will work with the [Wine Quality Dataset](https://archive.ics.uci.edu/ml/datasets/wine+quality). The goal of this lab is not to teach you a new classifier or even show you how to improve the performace of your existing model, but rather to help you streamline your machine learning workflows using scikit-learn pipelines. Pipelines let you keep your preprocessing and model building steps together, thus simplifying your cognitive load. You will see for yourself why pipelines are great by building the same KNN model twice, but in different ways. \n", "\n", "## Objectives \n", "\n", "- Construct pipelines in scikit-learn \n", "- Use pipelines in combination with `GridSearchCV()`\n", "\n", "## Import the data\n", "\n", "Run the following cell to import all the necessary classes, functions, and packages you need for this lab. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.model_selection import train_test_split, GridSearchCV\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.pipeline import Pipeline\n", "\n", "import warnings\n", "warnings.filterwarnings('ignore')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Import the `'winequality-red.csv'` dataset and print the first five rows of the data.  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Import the data\n", "df = None\n", "\n", "\n", "# Print the first five rows\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Use the `.describe()` method to print the summary stats of all columns in `df`. Pay close attention to the range (min and max values) of all columns. What do you notice? "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Print the summary stats of all columns\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As you can see from the data, not all features are on the same scale. Since we will be using k-nearest neighbors, which uses distance between features to classify points, we need to bring all these features to the same scale. This can be done using standardization. \n", "\n", "\n", "\n", "However, before standardizing the data, let's split it into training and test sets. \n", "\n", "> Note: You should always split the data before applying any scaling/preprocessing techniques in order to avoid data leakage. If you don't recall why this is necessary, you should refer to the **KNN with scikit-learn - Lab.** \n", "\n", "# Split the data \n", "\n", "- Assign the target (`'quality'` column) to `y` \n", "- Drop this column and assign all the predictors to `X` \n", "- Split `X` and `y` into 75/25 training and test sets. Set `random_state` to 42  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Split the predictor and target variables\n", "y = None\n", "X = None\n", "\n", "# Split into training and test sets\n", "X_train, X_test, y_train, y_test = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Standardize your data \n", "\n", "- Instantiate a `StandardScaler()` \n", "- Transform and fit the training data \n", "- Transform the test data "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Instantiate StandardScaler\n", "scaler = None\n", "\n", "# Transform the training and test sets\n", "scaled_data_train = None\n", "scaled_data_test = None\n", "\n", "# Convert into a DataFrame\n", "scaled_df_train = pd.DataFrame(scaled_data_train, columns=X_train.columns)\n", "scaled_df_train.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Train a model \n", "\n", "- Instantiate a `KNeighborsClassifier()` \n", "- Fit the classifier to the scaled training data "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Instantiate KNeighborsClassifier\n", "clf = None\n", "\n", "# Fit the classifier\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Use the classifier's `.score()` method to calculate the accuracy on the test set (use the scaled test data) "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Print the accuracy on test set\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Nicely done. This pattern (preprocessing and fitting models) is very common. Although this process is fairly straightforward once you get the hang of it, **pipelines** make this process simpler, intuitive, and less error-prone. \n", "\n", "Instead of standardizing and fitting the model separately, you can do this in one step using `sklearn`'s `Pipeline()`. A pipeline takes in any number of preprocessing steps each with `.fit()` and `transform()` methods (like `StandardScaler()` above), and a final step with a `.fit()` method (an estimator like `KNeighborsClassifier()`). The pipeline then sequentially applies preprocessing steps and finally fits the model. Do this now.   \n", "\n", "## Build a pipeline (I) \n", "\n", "Build a pipeline with two steps: \n", "\n", "- First step: `StandardScaler()` \n", "- Second step (estimator): `KNeighborsClassifier()` \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Build a pipeline with StandardScaler and KNeighborsClassifier\n", "scaled_pipeline_1 = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- Transform and fit the model using this pipeline to the training data (you should use `X_train` here) \n", "- Print the accuracy on test set (you should use `X_test` here) "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Fit the training data to pipeline\n", "\n", "\n", "# Print the accuracy on test set\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If you did everything right, this answer should match the one from above! \n", "\n", "Of course, you can also perform a grid search to determine which combination of hyperparameters can be used to build the best possible model. The way you define the pipeline still remains the same. What you need to do next is define the grid and then use `GridSearchCV()`. Let's do this now.\n", "\n", "## Build a pipeline (II)\n", "\n", "Again, build a pipeline with two steps: \n", "\n", "- First step: `StandardScaler()` \n", "- Second step (estimator): `RandomForestClassifier()`. Set `random_state=123` when instantiating the random forest classifier "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Build a pipeline with StandardScaler and RandomForestClassifier\n", "scaled_pipeline_2 = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Use the defined `grid` to perform a grid search. We limited the hyperparameters and possible values to only a few values in order to limit the runtime. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Define the grid\n", "grid = [{'RF__max_depth': [4, 5, 6], \n", "         'RF__min_samples_split': [2, 5, 10], \n", "         'RF__min_samples_leaf': [1, 3, 5]}]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define a grid search now. Use: \n", "- the pipeline you defined above (`scaled_pipeline_2`) as the estimator \n", "- the parameter `grid` \n", "- `'accuracy'` to evaluate the score \n", "- 5-fold cross-validation "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Define a grid search\n", "gridsearch = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["After defining the grid values and the grid search criteria, all that is left to do is fit it to training data and then score the test set. Do it below: "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Fit the training data\n", "\n", "\n", "# Print the accuracy on test set\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Summary"]}, {"cell_type": "markdown", "metadata": {}, "source": ["See how easy it is to define pipelines? Pipelines keep your preprocessing steps and models together, thus making your life easier. You can apply multiple preprocessing steps before fitting a model in a pipeline. You can even include dimensionality reduction techniques such as PCA in your pipelines. In a later section, you will work on this too! "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.3"}}, "nbformat": 4, "nbformat_minor": 2}